{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786a8f9e",
   "metadata": {},
   "source": [
    "# Kriss Sitapara NLTK\n",
    "\n",
    "N-Grams and Part of Speech Tagging using NLTK in Python. I will generate bi-grams and tri-grams from the text to count their frequencies, and identify the most common patterns. I will also use NLTK's `pos_tag()` to label each token with its part-of-speech. This helps in showing the grammatical structure of the text and help analyze patterns. There is also a small demonstration of how context serves an important role in tokenization and n-grams. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279d9c0",
   "metadata": {},
   "source": [
    "# Setup & Data\n",
    "Importing libraries and checking if NLTK resoruces work. Also importing the short description about Vancouver from **intext.txt**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0b39c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\kriss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing libraries\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "from collections import Counter\n",
    "nltk.download(\"tagsets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efb603a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whistler is a mounta\n"
     ]
    }
   ],
   "source": [
    "with open(\"intext.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a16bd1",
   "metadata": {},
   "source": [
    "# Tokenization & Normalization\n",
    "Tokenizing the text into sentences and words,normalizing , and stemming them using NLTK's PorterStemmer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97cb4730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 30\n",
      "Tokens: 726\n",
      "\n",
      "- Whistler is a mountain town and resort in British Columbia, Canada, about two hours north of Vancouver.\n",
      "- It sits in the Coast Mountains, in a valley surrounded by tall peaks, rivers, and lakes.\n",
      "- The land has been home to the Squamish and Lilâ€™wat First Nations for thousands of years, and both communities still have a presence in the area today.\n",
      "- They used the valley and nearby mountains for hunting, fishing, and trading long before the first settlers arrived.\n",
      "- The European settlers came much later, in the late 1800s, and the place began to develop as a small stop for trappers, prospectors, and travelers.\n",
      "\n",
      "Tokens: ['Whistler', 'is', 'a', 'mountain', 'town', 'and', 'resort', 'in', 'British', 'Columbia', ',', 'Canada', ',', 'about', 'two', 'hours', 'north', 'of', 'Vancouver', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokenizing sentences and words\n",
    "sentences = sent_tokenize(text)\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"Sentences: {len(sentences)}\")\n",
    "print(f\"Tokens: {len(tokens)}\\n\")\n",
    "\n",
    "\n",
    "#checking for correct output\n",
    "for s in sentences[:5]:\n",
    "    print(\"-\", s)\n",
    "print(\"\\nTokens:\", tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "918fdd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase Tokens:  {726}\n",
      "Alphabetic Tokens:  {618}\n",
      "Stemmed Tokens:  {618}\n"
     ]
    }
   ],
   "source": [
    "#normalizing and stemming\n",
    "lowercasetokens = [text.lower() for text in tokens]\n",
    "alphabetictokens = [text for text in lowercasetokens if text.isalpha()]\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(text) for text in alphabetictokens]\n",
    "\n",
    "#counting to make sure that the alphabetic tokens are less, since they are filtered versions of the lowercase tokens\n",
    "print(\"Lowercase Tokens: \", {len(lowercasetokens)})\n",
    "print(\"Alphabetic Tokens: \", {len(alphabetictokens)})\n",
    "print(\"Stemmed Tokens: \", {len(stems)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84adf55",
   "metadata": {},
   "source": [
    "I was a bit curious as to why lowercase included characters like \".\" or \",\" when clearly in grammar, we consider them as characters and punctuation marks. So I used a method called .isalpha() to get only the \"words\" to say. But after using this function, all the words with hyphen were not includeded such as \"sea-to-sky\", \"easy-access\" etc. In real world scenarios, the relevance of these words can differ by context, so for this current task I decided to let it be with just the .isalpha() check.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb40753",
   "metadata": {},
   "source": [
    "I was a bit curious and wanted to test this, so the following code is a possible implementation of this if interested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05448df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 726\n",
      "lowercase: 726\n",
      "alphabetic tokens '.isalpha()': 618\n",
      "regex tokens: 623\n",
      "\n",
      "tokens dropped by .isalpha() but kept by regex:\n",
      "['pedestrian-friendly', 'warm-weather', 'world-class', 'well-known', 'year-round']\n"
     ]
    }
   ],
   "source": [
    "#Custom versions of token filter to show the difference and possibly determine which one to use based on the words excluded.\n",
    "def testtoken(tokens):\n",
    "    \n",
    "    #lowercase version\n",
    "    ltoken = [t.lower() for t in tokens]\n",
    "    #.isalpha() version\n",
    "    atoken = [t for t in ltoken if t.isalpha()]\n",
    "    #custom regex version to include other characters like hyphen etc\n",
    "    rtoken = [t for t in ltoken if re.match(r\"^[a-zA-Z\\-']+$\", t)]\n",
    "    \n",
    "    \n",
    "    print(\"total tokens:\", len(tokens))\n",
    "    print(\"lowercase:\", len(ltoken))\n",
    "    print(\"alphabetic tokens '.isalpha()':\", len(atoken))\n",
    "    print(\"regex tokens:\", len(rtoken))\n",
    "    \n",
    "    #differences in excluded tokens\n",
    "    print(\"\\ntokens dropped by .isalpha() but kept by regex:\")\n",
    "    dropped = set(atoken) ^ set(rtoken) \n",
    "    print(list(dropped)[0:])\n",
    "    \n",
    "    return ltoken, atoken, rtoken\n",
    "\n",
    "ltoken, atoken, rtoken = testtoken(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0ff1d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some lowercased tokens:  [',', 'about', 'two', 'hours', 'north', 'of', 'vancouver', '.', 'it', 'sits', 'in', 'the', 'coast', 'mountains', ',', 'in', 'a', 'valley']\n",
      "\n",
      "some alpha-only tokens:  ['two', 'hours', 'north', 'of', 'vancouver', 'it', 'sits', 'in', 'the', 'coast', 'mountains', 'in', 'a', 'valley', 'surrounded', 'by', 'tall', 'peaks']\n",
      "\n",
      "some stemmed tokens:  ['two', 'hour', 'north', 'of', 'vancouv', 'it', 'sit', 'in', 'the', 'coast', 'mountain', 'in', 'a', 'valley', 'surround', 'by', 'tall', 'peak']\n"
     ]
    }
   ],
   "source": [
    "print(\"some lowercased tokens: \", lowercasetokens[12:30])\n",
    "print(\"\\nsome alpha-only tokens: \", alphabetictokens[12:30])\n",
    "print(\"\\nsome stemmed tokens: \", stems[12:30])\n",
    "\n",
    "#I wonder why something like a \",\" or \".\" is considered a lowercase token. Shouldnt it be removed since it isnt really \"lowercase\"?\n",
    "#Another thing that I noticed was that the stemming behaved a bit odd. Words like tired became tire, and quads became quad. Is this to avoid grammer tenses?\n",
    "#Some words like activities, activity, active all became activ, so that they can be grouped in the same category of \"active\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b310f",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "Using NLTK's `pos_tag()` to label each token with its part-of-speech. This helps in showing the grammatical structure of the text and help analyze patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98f44a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some POS-tagged tokens:\n",
      "[('whistler', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('mountain', 'NN'), ('town', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "postags = pos_tag(alphabetictokens)\n",
    "print(\"Some POS-tagged tokens:\")\n",
    "print(postags[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52976bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags:\n",
      "\n",
      "NN - 129\n",
      "IN - 78\n",
      "DT - 77\n",
      "NNS - 67\n",
      "JJ - 55\n",
      "CC - 40\n",
      "VBD - 26\n",
      "RB - 19\n",
      "VBN - 17\n",
      "TO - 16\n",
      "VBZ - 14\n",
      "VBG - 14\n",
      "VB - 13\n",
      "VBP - 11\n",
      "PRP - 10\n",
      "JJS - 5\n",
      "CD - 4\n",
      "WDT - 4\n",
      "PRP$ - 4\n",
      "RBR - 3\n",
      "WRB - 3\n",
      "JJR - 2\n",
      "WP - 2\n",
      "RP - 2\n",
      "MD - 1\n",
      "RBS - 1\n",
      "EX - 1\n"
     ]
    }
   ],
   "source": [
    "tcount = Counter(tag for word, tag in postags)\n",
    "\n",
    "print(\"POS tags:\\n\")\n",
    "for tag, count in tcount.most_common():\n",
    "    print(f\"{tag} - {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ded3b616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General POS categories:\n",
      "\n",
      "Nouns - 196\n",
      "Verbs - 95\n",
      "Prepositions - 78\n",
      "Determiners - 77\n",
      "Adjectives - 62\n",
      "Conjunctions - 40\n",
      "Other - 30\n",
      "Adverbs - 23\n",
      "Pronouns - 16\n",
      "Modals - 1\n"
     ]
    }
   ],
   "source": [
    "group = {\n",
    "    \"NN\": \"Nouns\", \"NNS\": \"Nouns\", \"NNP\": \"Nouns\", \"NNPS\": \"Nouns\",\n",
    "    \"VB\": \"Verbs\", \"VBD\": \"Verbs\", \"VBG\": \"Verbs\", \"VBN\": \"Verbs\", \"VBP\": \"Verbs\", \"VBZ\": \"Verbs\",\n",
    "    \"JJ\": \"Adjectives\", \"JJR\": \"Adjectives\", \"JJS\": \"Adjectives\",\n",
    "    \"RB\": \"Adverbs\", \"RBR\": \"Adverbs\", \"RBS\": \"Adverbs\",\n",
    "    \"DT\": \"Determiners\",\n",
    "    \"IN\": \"Prepositions\",\n",
    "    \"CC\": \"Conjunctions\",\n",
    "    \"PRP\": \"Pronouns\", \"PRP$\": \"Pronouns\", \"WP\": \"Pronouns\", \"WP$\": \"Pronouns\",\n",
    "    \"MD\": \"Modals\",\n",
    "}\n",
    "\n",
    "\n",
    "gcount = Counter()\n",
    "for tag, count in tcount.items():\n",
    "    category = group.get(tag, \"Other\")\n",
    "    gcount[category] += count\n",
    "\n",
    "\n",
    "print(\"General POS categories:\\n\")\n",
    "for category, count in gcount.most_common():\n",
    "    print(f\"{category} - {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807cd50",
   "metadata": {},
   "source": [
    "By the looks of this data, it seems to be pretty accurate. Since my original text is about Whistler and Vancouver, it makes sense that a places and attractions are mentioned in it. There are a lot of adjectives as well which makes sense, since they are used to describe nouns. Another thing that I noticed is something like \"Whistler\" or \"Blackcomb\" is just tagged as NN(common nouns) instead of NNP(proper nouns). One huge reason why I suspect this is because the tokens extracted are lowercased, therefore the POS tagger will mark it as NN instead of NNP. Also, some words(for example - early) can be tagged as either RB(adverb) or JJ(adjective ) depending on sentence position, so I wonder how the pos_tag() determines this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980245e",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "Generating bi-grams and tri-grams from the text to count their frequencies, and identify the most common patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd783b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top bigrams:\n",
      "('in', 'the') - 8\n",
      "('of', 'the') - 6\n",
      "('the', 'area') - 3\n",
      "('and', 'the') - 3\n",
      "('at', 'the') - 3\n",
      "('it', 'was') - 3\n",
      "('a', 'mountain') - 2\n",
      "('mountain', 'town') - 2\n",
      "('of', 'vancouver') - 2\n",
      "('in', 'a') - 2\n",
      "\n",
      "top trigrams:\n",
      "('a', 'mountain', 'town') - 2\n",
      "('for', 'the', 'winter') - 2\n",
      "('the', 'winter', 'olympics') - 2\n",
      "('is', 'one', 'of') - 2\n",
      "('one', 'of', 'the') - 2\n",
      "('whistler', 'and', 'blackcomb') - 2\n",
      "('whistler', 'is', 'a') - 1\n",
      "('is', 'a', 'mountain') - 1\n",
      "('mountain', 'town', 'and') - 1\n",
      "('town', 'and', 'resort') - 1\n"
     ]
    }
   ],
   "source": [
    "#grams using the alphabetic tokens\n",
    "bigrams = list(ngrams(alphabetictokens, 2))\n",
    "trigrams = list(ngrams(alphabetictokens, 3))\n",
    "bigramf = FreqDist(bigrams)\n",
    "trigramf = FreqDist(trigrams)\n",
    "\n",
    "#results\n",
    "print(\"top bigrams:\")\n",
    "for pair, count in bigramf.most_common(10):\n",
    "    print(f\"{pair} - {count}\")\n",
    "print(\"\\ntop trigrams:\")\n",
    "for triplet, count in trigramf.most_common(10):\n",
    "    print(f\"{triplet} - {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "572de68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top bigrams:\n",
      "('in', 'the') - 8\n",
      "('of', 'the') - 6\n",
      "('the', 'area') - 3\n",
      "('and', 'the') - 3\n",
      "('at', 'the') - 3\n",
      "('the', 'mountain') - 3\n",
      "('it', 'wa') - 3\n",
      "('the', 'ski') - 3\n",
      "('a', 'mountain') - 2\n",
      "('mountain', 'town') - 2\n",
      "\n",
      "top trigrams:\n",
      "('a', 'mountain', 'town') - 2\n",
      "('for', 'the', 'winter') - 2\n",
      "('the', 'winter', 'olymp') - 2\n",
      "('is', 'one', 'of') - 2\n",
      "('one', 'of', 'the') - 2\n",
      "('whistler', 'and', 'blackcomb') - 2\n",
      "('whistler', 'is', 'a') - 1\n",
      "('is', 'a', 'mountain') - 1\n",
      "('mountain', 'town', 'and') - 1\n",
      "('town', 'and', 'resort') - 1\n"
     ]
    }
   ],
   "source": [
    "#grams using the stemmed tokens\n",
    "bigrams = list(ngrams(stems, 2))\n",
    "trigrams = list(ngrams(stems, 3))\n",
    "bigramf = FreqDist(bigrams)\n",
    "trigramf = FreqDist(trigrams)\n",
    "\n",
    "#results\n",
    "print(\"top bigrams:\")\n",
    "for pair, count in bigramf.most_common(10):\n",
    "    print(f\"{pair} - {count}\")\n",
    "print(\"\\ntop trigrams:\")\n",
    "for triplet, count in trigramf.most_common(10):\n",
    "    print(f\"{triplet} - {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1a612",
   "metadata": {},
   "source": [
    "This was a pretty interesting find. The most common bigrams were words like \"in the\", \"of the\", \"the area\". I tried creating grams for both the alphabetic tokens and the stemmed tokens because I had a feeling that there will be some kind of difference but didnt know what exactly.  The trigrams were same for both the tokens, but the major difference was in the bigrams. I noticed that Vancouver was mentioned in the bigram for the alphabetic tokens but not in the stemmed tokens. This is pretty interesting, because although Vancouver is a proper noun and serves more generality and importance here in the text, it gets stemmed to vancouv which is not a proper noun anymore. This means that other stemmed words appeared more earlier and frequently than \"of vancouver\".  Imagine you had two words such as \"the car\" and the \"the carousel\". In this case even though both are different words, the stemmed version of carousel can possibly be \"the car\" which can cause confusion and misinterpretation. This means that sometimes you need to be careful on how to tokenize based on the context of the text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
